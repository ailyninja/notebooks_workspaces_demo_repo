{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8ff7c2ed",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "\n# Predicting U.S. Unemployment Rate from synthetic ADP Non-Farm Payrolls with PyTorch (GPU)\n\nThis sample notebook walks through a minimal-yet-complete forecasting workflow—from **data loading & cleaning** to **model training** and **inference**—using **PyTorch**. It uses GPU acceleration automatically if available.\n\n**What you'll see:**\n1. Data loading \n2. Cleaning, alignment, and feature engineering (lags, rolling means, calendar)\n3. Train/validation split that respects time order\n4. Scaling\n5. PyTorch `Dataset` / `DataLoader`\n6. A compact MLP forecaster (you could swap for LSTM/GRU)\n7. Training loop with early stopping\n8. Evaluation and error metrics\n9. Saving, reloading, and running inference on a \"next month\" example\n\n> **Note**: This example uses ADP non-farm payrolls as the primary driver. In practice, adding more macro features (e.g., continuing claims, JOLTS, ISM, CPI) typically improves performance."
    },
    {
      "id": "5b5f0a59-e0c9-4c58-adfc-62bd2bf0748b",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "\n# Install instructions (if running locally and PyTorch isn't available):\n#   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n#   pip install pandas numpy scikit-learn matplotlib\n\nimport math\nimport os\nimport sys\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\ntry:\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import Dataset, DataLoader\nexcept Exception as e:\n    raise RuntimeError(\n        \"PyTorch is required to run this notebook. Please install it first. \"\n        \"See installation hint in the cell above.\"\n    ) from e\n\nimport matplotlib.pyplot as plt\n\n# Select device (prefer GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "571c39bc",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "\n## 1) Load data \n\nWe'll generate a synthetic monthly dataset with columns:\n\n- `date` (month-end)\n- `adp_change` (ADP non-farm payroll change, thousands)\n- `unemployment_rate` (percent)\n\n**Expected CSV schema (if you bring your own):**\n```csv\ndate,adp_change,unemployment_rate\n2010-01-31,45,9.8\n2010-02-28,130,9.8\n...\n```"
    },
    {
      "id": "4cd7c592-498e-4b6a-9dca-f3b7711717c1",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "def synthesize_data(n_months=180, seed=42):\n    rng = np.random.default_rng(seed)\n    start = pd.Timestamp(\"2010-01-31\")\n    dates = pd.date_range(start=start, periods=n_months, freq=\"M\")\n\n    # ADP hires pattern: base seasonal + noise\n    seasonal = 150 * np.sin(np.linspace(0, 8*np.pi, n_months))  # seasonal swing\n    trend = np.linspace(50, 250, n_months)                      # slow drift upward\n    noise = rng.normal(0, 80, size=n_months)\n    adp_change = (seasonal + trend + noise).astype(float)\n\n    # Unemployment rate: depends negatively on ADP hiring, with lag + mean reversion\n    ur = np.empty(n_months)\n    ur[0] = 8.5\n    for t in range(1, n_months):\n        # Higher ADP -> lower UR next month; also add small persistence\n        ur[t] = 0.85 * ur[t-1] - 0.0009 * adp_change[t-1] + rng.normal(0, 0.08)\n        # Bound realistically\n        ur[t] = float(np.clip(ur[t], 2.5, 12.0))\n\n    df = pd.DataFrame({\n        \"date\": dates,\n        \"adp_change\": adp_change,\n        \"unemployment_rate\": ur\n    })\n    return df\n\ndf = synthesize_data()\n\ndf.head(5)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "f60dc5f4",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "\n## 2) Clean & feature engineer\n\nWe'll create:\n- **Lag features** for ADP and unemployment rate (predict next month's UR from recent months).\n- **Rolling means** for ADP to smooth noise.\n- **Calendar features** (month as a cyclical variable).\n\nWe'll predict **next month's** unemployment rate (`target_t+1`)."
    },
    {
      "id": "6b15c7bd-fb2d-4238-acc6-03d8bcd1f505",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "\ndef add_features(dfin, adp_lags=(1,2,3), ur_lags=(1,2), adp_rolls=(3,6)):\n    df = dfin.copy()\n    df = df.sort_values(\"date\").reset_index(drop=True)\n\n    # Lag features\n    for L in adp_lags:\n        df[f\"adp_lag{L}\"] = df[\"adp_change\"].shift(L)\n    for L in ur_lags:\n        df[f\"ur_lag{L}\"] = df[\"unemployment_rate\"].shift(L)\n\n    # Rolling means of ADP\n    for W in adp_rolls:\n        df[f\"adp_roll{W}\"] = df[\"adp_change\"].rolling(W).mean()\n\n    # Calendar cyclic features\n    df[\"month\"] = df[\"date\"].dt.month\n    df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12.0)\n    df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12.0)\n\n    # Target is next month's UR\n    df[\"target_next_ur\"] = df[\"unemployment_rate\"].shift(-1)\n\n    # Drop rows with NA due to lags/rolls/shift\n    df = df.dropna().reset_index(drop=True)\n    return df\n\ndf_feat = add_features(df)\ndf_feat.tail(3)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "052c98cd",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "\n## 3) Train/Validation split (time-aware)\n\nWe keep chronological order to avoid leakage. We'll use the last 15% of samples as validation."
    },
    {
      "id": "b612fe9e-7beb-4bc7-9dd5-cac12ae11690",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "val_frac = 0.15\nn_total = len(df_feat)\nn_val = int(round(n_total * val_frac))\nn_train = n_total - n_val\n\ntrain_df = df_feat.iloc[:n_train].reset_index(drop=True)\nval_df = df_feat.iloc[n_train:].reset_index(drop=True)\n\nn_train, n_val",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "6136b58a",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "\n## 4) Scale features\n\nWe'll standardize features using statistics from the **training** set only. Targets are left in original units (percentage points)."
    },
    {
      "id": "b895815b-9e4a-4272-ac55-25ee85ed79e6",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "from sklearn.preprocessing import StandardScaler\n\nfeature_cols = [c for c in df_feat.columns if c not in [\"date\",\"unemployment_rate\",\"target_next_ur\",\"month\"]]\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(train_df[feature_cols].values)\nX_val = scaler.transform(val_df[feature_cols].values)\n\ny_train = train_df[\"target_next_ur\"].values.astype(np.float32)\ny_val = val_df[\"target_next_ur\"].values.astype(np.float32)\n\nX_train.shape, X_val.shape, y_train.shape, y_val.shape",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "0dcbcf4d",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "\n## 5) PyTorch `Dataset` & `DataLoader`\n\nWe'll create a simple tabular dataset for supervised regression."
    },
    {
      "id": "20d547eb-3c8e-422a-839f-15f10099b4d5",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "class TabularDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n\n    def __len__(self):\n        return self.X.shape[0]\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\nbatch_size = 32\ntrain_ds = TabularDataset(X_train, y_train)\nval_ds = TabularDataset(X_val, y_val)\n\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\nval_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n\nlen(train_ds), len(val_ds)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "4a56821e",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "\n## 6) Model: Compact MLP forecaster\n\nA small MLP is adequate for this demo. You can replace it with an LSTM/GRU if you transform the problem into a sequence model."
    },
    {
      "id": "472df285-2158-4960-b2e8-3dd4d036a071",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "class MLPRegressor(nn.Module):\n    def __init__(self, in_features, hidden=(64, 32), dropout=0.1):\n        super().__init__()\n        layers = []\n        last = in_features\n        for h in hidden:\n            layers.append(nn.Linear(last, h))\n            layers.append(nn.ReLU())\n            layers.append(nn.Dropout(dropout))\n            last = h\n        layers.append(nn.Linear(last, 1))\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n\nmodel = MLPRegressor(in_features=X_train.shape[1]).to(device)\ncriterion = nn.MSELoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "5763e7fb",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "\n## 7) Training loop with early stopping"
    },
    {
      "id": "77c91285-0aff-4c2e-b159-7fc466c18460",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "def evaluate(model, loader, device):\n    model.eval()\n    total_loss = 0.0\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb = xb.to(device)\n            yb = yb.to(device)\n            pred = model(xb)\n            loss = criterion(pred, yb)\n            total_loss += loss.item() * xb.size(0)\n    return total_loss / len(loader.dataset)\n\nbest_state = None\nbest_val = float(\"inf\")\npatience = 20\npat = 0\nnum_epochs = 300\ntrain_losses, val_losses = [], []\n\nfor epoch in range(1, num_epochs + 1):\n    model.train()\n    running = 0.0\n    for xb, yb in train_loader:\n        xb = xb.to(device)\n        yb = yb.to(device)\n        optimizer.zero_grad()\n        pred = model(xb)\n        loss = criterion(pred, yb)\n        loss.backward()\n        optimizer.step()\n        running += loss.item() * xb.size(0)\n    train_loss = running / len(train_loader.dataset)\n    val_loss = evaluate(model, val_loader, device)\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n\n    if val_loss < best_val - 1e-6:\n        best_val = val_loss\n        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n        pat = 0\n    else:\n        pat += 1\n\n    if epoch % 25 == 0 or epoch == 1:\n        print(f\"Epoch {epoch:03d} | train {train_loss:.4f} | val {val_loss:.4f}\")\n    if pat >= patience:\n        print(f\"Early stopping at epoch {epoch}. Best val: {best_val:.4f}\")\n        break\n\n# Load best weights\nif best_state is not None:\n    model.load_state_dict(best_state)\nmodel.to(device)\n\nbest_val",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "365f3c5d",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "\n## 8) Learning curves"
    },
    {
      "id": "021ba1f6-6d93-4f0c-924a-21aa881d3f2a",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "plt.figure(figsize=(6,4))\nplt.plot(train_losses, label=\"train\")\nplt.plot(val_losses, label=\"val\")\nplt.title(\"Learning Curves (MSE)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "8bca0b5c",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "\n## 9) Metrics & sanity check predictions\nWe'll report RMSE and a quick actual-vs-predicted plot on the validation set."
    },
    {
      "id": "86bbea3b-3bb0-434c-a1df-96cbc992463e",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "from sklearn.metrics import root_mean_squared_error\n\nmodel.eval()\nwith torch.no_grad():\n    Xv = torch.tensor(X_val, dtype=torch.float32).to(device)\n    pv = model(Xv).cpu().numpy().ravel()\n\nrmse = root_mean_squared_error(y_val, pv)\nprint(\"Validation RMSE (pct-pts):\", rmse)\n\nplt.figure(figsize=(6,4))\nplt.plot(val_df[\"date\"], y_val, label=\"Actual (next UR)\")\nplt.plot(val_df[\"date\"], pv, label=\"Predicted (next UR)\")\nplt.title(\"Validation: Next-Month Unemployment Rate\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"%\")\nplt.legend()\nplt.xticks(rotation=30)\nplt.tight_layout()\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "c6f2964c",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "\n## 10) Save scaler & model for later inference"
    },
    {
      "id": "2bc10cf9-b6e2-44ff-b6b6-198773b49846",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "save_dir = \"/mnt/data\"\nos.makedirs(save_dir, exist_ok=True)\n\ntorch.save(model.state_dict(), os.path.join(save_dir, \"ur_from_adp_mlp.pt\"))\nimport joblib\njoblib.dump(scaler, os.path.join(save_dir, \"ur_from_adp_scaler.joblib\"))\n\nprint(\"Saved:\", os.path.join(save_dir, \"ur_from_adp_mlp.pt\"))\nprint(\"Saved:\", os.path.join(save_dir, \"ur_from_adp_scaler.joblib\"))",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "41014866",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "\n## 11) Inference (predicting the **next** month)\n\nHere we construct the latest available feature row from the dataset (or your CSV), transform it with the **training scaler**, and generate a prediction for `target_next_ur` (next-month unemployment rate)."
    },
    {
      "id": "45df18e9-0ae1-464e-8bf6-3706a8d59431",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "\n# Rebuild the newest feature row from the original (unscaled) feature DataFrame.\nlatest_row = df_feat.iloc[[-1]].copy()\n\n# Prepare the feature vector in correct column order\nX_new = latest_row[feature_cols].values.astype(np.float32)\n\n# Scale with training scaler\nX_new_scaled = scaler.transform(X_new)\n\n# Torch tensor -> device\nX_new_t = torch.tensor(X_new_scaled, dtype=torch.float32).to(device)\n\nmodel.eval()\nwith torch.no_grad():\n    pred_next_ur = model(X_new_t).cpu().numpy().ravel()[0]\n\nprint(f\"Predicted next-month unemployment rate: {pred_next_ur:.2f}%\")\nprint(\"This prediction corresponds to the month after:\", latest_row['date'].dt.strftime('%Y-%m').item())",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "38745041",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "\n---\n\n## Appendix: Bring-your-own ADP/BLS CSVs\n\n1. **ADP file** (monthly):  \n   - Columns: `date, adp_change`  \n   - `date` should be end-of-month (or you can convert to month-end with `pd.to_datetime(...).dt.to_period('M').dt.to_timestamp('M')`).\n\n2. **Unemployment rate file** (monthly, from BLS `LNS14000000` or similar):  \n   - Columns: `date, unemployment_rate` (percent).\n\n3. **Join & save** to `/mnt/data/adp_unemployment_sample.csv` with columns:\n   ```csv\n   date,adp_change,unemployment_rate\n   2010-01-31,45,9.8\n   ...\n   ```\n\n4. **Rerun** the notebook; it will pick up your CSV automatically.\n\n> For sequence models (LSTM/GRU), arrange windows of past `k` months as inputs; the rest of the pipeline remains similar."
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}